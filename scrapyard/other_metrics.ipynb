{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1661364 records read\n",
      "618447 records\n",
      "5572 unique metrics\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "os.environ['DATABASE_URL'] = 'postgres:///awrdata'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir('/users/dane/src/datatools/ipy/notebooks/pt/dev/')\n",
    "sys.path.append('/users/dane/src/datatools/analytics/')\n",
    "\n",
    "import pertest.util\n",
    "from pertest.util import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data is linked to ~/data\n",
    "df = pd.read_csv('../data/v14_b60-73.csv')\n",
    "df['value'] = df['t_value']  # required for level_shift\n",
    "print(len(df), 'records read')\n",
    "# keep only time based metric data\n",
    "odf = df[df.m_name.apply(lambda x: ('$T' not in x) and ('$M' not in x))]\n",
    "print(len(odf), 'records')\n",
    "print(len(odf.m_id.unique()), 'unique metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Time/Memory Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f15mean</th>\n",
       "      <th>f15median</th>\n",
       "      <th>f15range</th>\n",
       "      <th>l15mean</th>\n",
       "      <th>l15median</th>\n",
       "      <th>m_id</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>name</th>\n",
       "      <th>range</th>\n",
       "      <th>test_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AxPerf:Freqs_Sim</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Microstrip_to_Stripline@Microstrip_to_Striplin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72883</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>AxPerf:Iterations</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Microstrip_to_Stripline@Microstrip_to_Striplin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AxPerf:Sim Mode</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Microstrip_to_Stripline@Microstrip_to_Striplin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72893</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>AxPerf:Res_Threads</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Microstrip_to_Stripline@Microstrip_to_Striplin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>72898</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>AxPerf:Freqs_Sim</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nRT8_Stripline_DeEmbedding_Single_Trace_Finite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f15mean  f15median  f15range  l15mean  l15median   m_id   max  mean  \\\n",
       "0      1.0        1.0       0.0      1.0        1.0  72873   1.0   1.0   \n",
       "1     67.0       67.0       0.0     67.0       67.0  72883  67.0  67.0   \n",
       "2      0.0        0.0       0.0      0.0        0.0  72889   0.0   0.0   \n",
       "3      8.0        8.0       0.0      8.0        8.0  72893   8.0   8.0   \n",
       "4      6.0        6.0       0.0      6.0        6.0  72898   6.0   6.0   \n",
       "\n",
       "   median   min                name  range  \\\n",
       "0     1.0   1.0    AxPerf:Freqs_Sim    0.0   \n",
       "1    67.0  67.0   AxPerf:Iterations    0.0   \n",
       "2     0.0   0.0     AxPerf:Sim Mode    0.0   \n",
       "3     8.0   8.0  AxPerf:Res_Threads    0.0   \n",
       "4     6.0   6.0    AxPerf:Freqs_Sim    0.0   \n",
       "\n",
       "                                           test_case  \n",
       "0  Microstrip_to_Stripline@Microstrip_to_Striplin...  \n",
       "1  Microstrip_to_Stripline@Microstrip_to_Striplin...  \n",
       "2  Microstrip_to_Stripline@Microstrip_to_Striplin...  \n",
       "3  Microstrip_to_Stripline@Microstrip_to_Striplin...  \n",
       "4  nRT8_Stripline_DeEmbedding_Single_Trace_Finite...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build metric dataframe\n",
    "tdf = odf.sort_values(by='build')  # make sure it's in the right order\n",
    "d = []\n",
    "for m_id, items in tdf.groupby('m_id'):\n",
    "    rec = items.iloc[0]\n",
    "    f15 = items[:15]\n",
    "    l15 = items[-15:]\n",
    "    f15m = f15.t_value.median()\n",
    "    m = items.t_value.median()\n",
    "    # do all points fall within .1 of mean\n",
    "    avg = items.t_value.mean()\n",
    "    mx = items.t_value.max()\n",
    "    mn = items.t_value.min()\n",
    "    minfo = dict(\n",
    "        m_id=rec.m_id,\n",
    "        name=rec.m_name,\n",
    "        test_case=rec.test_case,\n",
    "        f15median=f15m,\n",
    "        f15mean=f15.t_value.mean(),\n",
    "        f15range=f15.t_value.max() - f15.t_value.min(),\n",
    "        l15median=l15.t_value.median(),\n",
    "        l15mean=l15.t_value.mean(),\n",
    "        median=m,\n",
    "        mean=avg,\n",
    "        max=mx,\n",
    "        min=mn,\n",
    "        range=mx - mn,\n",
    "    )\n",
    "    d.append(minfo)\n",
    "metrics = pd.DataFrame(d)\n",
    "pertest.util.df = odf\n",
    "pertest.util.metrics = metrics\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other Metrics\n",
    "\n",
    "### Fixed value metrics\n",
    "\n",
    "Several other parameters such as Model, Ncp, Np, Order and Status don't have specific values but should not change and will be MEAN_EX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_settings = {}\n",
    "for m_id, items in odf.groupby('m_id'):\n",
    "    row = items.iloc[0]\n",
    "    (file, name) = row.m_name.split(':')\n",
    "    if ('$H' in name) or (name in ['Model', 'Ncp', 'Np', 'Order', 'Status', 'Iterations']):\n",
    "        metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                                     meas_type='MEAN_EX', mean=row.t_value)\n",
    "len(metric_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metrics\n",
    "\n",
    "Some metrics are errors.  In this case, we have an upper bound of -30 and an error tolerance of +/- 15.  Some of these metrics are > 0 which should be illegal so those will be made pass metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Num': 1020, 'Failed window': 95})\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "x = []\n",
    "for m_id, items in odf.groupby('m_id'):\n",
    "    row = items.iloc[0]\n",
    "    (file, name) = row.m_name.split(':')\n",
    "    if name in ['Max_err', 'RMS_err', 'Rfun0', 'Rfun1', 'Rfun2', 'Rfun3']:\n",
    "        c['Num'] += 1\n",
    "        v1 = row.t_value\n",
    "        if v1 == 0:\n",
    "            failed = items[items.t_value !=0]\n",
    "            if len(failed) > 0:\n",
    "                c['Failed 0'] += 1\n",
    "            metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                             meas_type='VAL_EX', value=0)\n",
    "        elif v1 > 0:\n",
    "            # need to know what should really happen to these\n",
    "            metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                         meas_type='PASS')\n",
    "        else:\n",
    "            f15mean = items[:15].t_value.mean()\n",
    "            upper = min(-30, f15mean+10)\n",
    "            lower = f15mean - 10\n",
    "            failed = items[(items.t_value > upper) | (items.t_value < lower)]\n",
    "            if len(failed) > 0:\n",
    "                x.append(m_id)\n",
    "                c['Failed window'] += 1\n",
    "            metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                             meas_type='MEAN_ABS', mean=f15mean, upper=(upper-f15mean), lower=(f15mean-lower))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3604"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metric_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the phase errors:\n",
    "\n",
    "* If the value is 0, AFS did not run so the value should remain 0\n",
    "* Any value > 0.2 is a failure or any change greater than 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Num': 170, 'Failed window': 1})\n"
     ]
    }
   ],
   "source": [
    "c = Counter()\n",
    "x = []\n",
    "for m_id, items in odf.groupby('m_id'):\n",
    "    row = items.iloc[0]\n",
    "    (file, name) = row.m_name.split(':')\n",
    "    if name == 'Ph_err':\n",
    "        c['Num'] += 1\n",
    "        v1 = row.t_value\n",
    "        if v1 == 0:\n",
    "            failed = items[items.t_value !=0]\n",
    "            if len(failed) > 0:\n",
    "                c['Failed 0'] += 1\n",
    "            metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                         meas_type='VAL_EX', value=0)\n",
    "        else:\n",
    "            f15mean = items[:15].t_value.mean()\n",
    "            upper = min(.2, f15mean+.05)\n",
    "            lower = f15mean - .05\n",
    "            failed = items[(items.t_value > upper) | (items.t_value < lower)]\n",
    "            metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case, current_sequence=0,\n",
    "                 meas_type='MEAN_ABS', mean=f15mean, upper=(upper-f15mean), lower=(f15mean-lower))\n",
    "            if len(failed) > 0:\n",
    "                x.append(m_id)\n",
    "                c['Failed window'] += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3774"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metric_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused metrics\n",
    "\n",
    "the V_c and V_i metrics are not used and will be set to PASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for m_id, items in odf.groupby('m_id'):\n",
    "    row = items.iloc[0]\n",
    "    (file, name) = row.m_name.split(':')\n",
    "    if name.startswith('V_'):\n",
    "        metric_settings[m_id] = dict(id=int(m_id), name=row['m_name'], test_case=row.test_case,\n",
    "                                     current_sequence=0, meas_type='PASS')\n",
    "len(metric_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Based on the rules supplied by Pekka we will set the metrics as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3980\n"
     ]
    }
   ],
   "source": [
    "data = list(metric_settings.values())\n",
    "print(type(data))\n",
    "print(len(data))\n",
    "with open('other_settings.json', 'w') as fp:\n",
    "    fp.write(json.dumps(data, indent=2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
